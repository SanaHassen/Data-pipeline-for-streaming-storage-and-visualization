version: '3'

services:
  zookeeper:
    image: bitnami/zookeeper
    environment:
    - ALLOW_ANONYMOUS_LOGIN=yes 
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "zookeeper-shell", "localhost:2181", "ls /"]
      interval: 5s
      timeout: 5s
      retries: 3

  kafka:
    image: bitnami/kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      ALLOW_PLAINTEXT_LISTENER: "yes"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "/opt/bitnami/kafka/bin/kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 5s
      timeout: 5s
      retries: 5


  hdfs-namenode:
    image: sequenceiq/hadoop-docker
    command: /etc/bootstrap.sh -d namenode
    ports:
      - "50070:50070"
      - "9000:9000"
    expose:
      - "50075"
    healthcheck:
      test: ["bash", "-c", "if [[ $(hdfs dfsadmin -safemode get | grep 'Safe mode is ON') ]]; then exit 1; else exit 0; fi"]
      interval: 30s
      timeout: 5s
      retries: 3

  hdfs-datanode:
    image: sequenceiq/hadoop-docker
    command: /etc/bootstrap.sh -d datanode
    links:
      - hdfs-namenode
    volumes:
      - /var/hdfs/data
    expose:
      - "50010"
      - "50020"
      - "50090"
      - "50075"



  add-spark-permissions-to-hdfs:
    image: alpine:latest
    command: sh -c "apk add --no-cache bash && docker exec hdfs-namenode /usr/local/hadoop/bin/hdfs dfs -chown -R spark:spark /"
    depends_on:
      hdfs-namenode:
        condition: service_healthy


  streaming-app:
    build: ./streaming_app
    depends_on:
      kafka:
        condition: service_healthy
      add-spark-permissions-to-hdfs:
        condition: service_healthy



    




  # New service for the dashboard
  dashboard:
    build: ./dashboard  # Path to the directory containing your dashboard code
    ports:
      - "8050:8050"  # Assuming Dash runs on port 8050
    depends_on:
      streaming-app:
        condition: service_healthy
      csv-to-kafka:
        condition: service_completed_successfully

  csv-to-kafka:
    build: ./csv_produce_kafka
    depends_on:
      kafka:
        condition: service_healthy

